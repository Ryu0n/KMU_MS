# 순환 신경망 (Recurrent Neural Network)
## 시간성 데이터 (time series data), 순차적 데이터 (sequential data)
이전까지 다룬 데이터들은 정적인 데이터고, 길이가 고정적이다. 반면에, 시간성 데이터는 동적이면서 길이가 가변적이다.
주로, 시간의 순서에 따라 나열된 데이터들을 의미한다.  
![img.png](img.png)

## 순차적 데이터 예시
![img_1.png](img_1.png)  

## 순차적 데이터 표기
![img_2.png](img_2.png)  
x는 순차적 데이터의 집합을 의미하며, 집합의 각 원소는 시간에 따라 순차적인 데이터 값을 나열한 것이다. 
해당 원소는 데이터의 차원수에 해당하는 데이터를 포함하고 있다. 그리고, 이들의 집합을 **전치**를 취한다.  
중요한 점은 **집합도 벡터를 의미하며, 집합의 원소들 또한 벡터라는 점이다.** (결국 벡터화를 해주어야 한다..)

## 순차적 데이터의 특징  
![img_4.png](img_4.png)  

* **특징이 나타내는 순서가 중요**  
  예를 들어, "아빠가 방에 들어간다."라는 문장을 "아빠 가방에 들어간다."로 순서를 변형하면 의미가 크게 회손된다.


* **샘플마다 길이가 다름**  
  순환 신경망은 **은닉층에 순환 연결**을 부여하여 가변 길이 수용


* **문맥 의존성 (context dependency)**  
  비순차 데이터는 공분산이 특징 사이의 의존성을 나타냄. (지역성을 띄는 이미지같은 경우에는 한 시점의 서로 다른 요소들 사이에 영향이 공분산)  
  반면, 순차 데이터는 공분산이 의미가 없다. 대신 문맥 의존성이 중요하다. (현시점에서만 보면 서로 다른 요소가 관계를 띄는것 처럼 보일지 모르지만 이후의 시점에서는 서로 영향을 준다는 보장이 없음)  
  ex) “그녀는 점심때가 다 되어서야 .... 점심을 먹었는데, 철수는 ...”에서 “그녀는”과 “먹었는데”는 강한 문맥 의존성을 가짐  
  특히, 이 경우 둘 사이의 간격이 크므로 장기 의존성이라 부름 > LSTM 으로 처리

## Word Embedding
![img_3.png](img_3.png)  
위에서 보았다시피, 순차적 데이터 또한 신경망에 학습시키려면 **벡터화**과정을 거쳐야 한다. 그래서 고차원의 단어들을 저차원의 공간으로 변환하는 과정을 의미한다.


## RNN(Recurrent Neural Network) 과 LSTM(Long-Short Term Memory)
순환 신경망은 주로 **시간성 정보**를 활용하여 순차 데이터를 처리하는 효과적인 학습 모델이다. 그러나, 데이터의 길이가 길어질수록
CNN의 Grandient Vanishing 현상처럼 RNN에서도 기억을 잃는 현상이 있다. 이를 보완한 모델이 LSTM이다.  

최근에는 순환 신경망을 생성 모델로 사용  
▪ 예, CNN과 LSTM이 협력하여 자연 영상에 주석 생성하는 문제를 해결

### RNN 구조
![img_5.png](img_5.png)  
**입력층, 은닉층, 출력층**의 구조를 지니고 있으며 은닉층이 **순환연결**구조 를 가지고 있다.
ht-1(hidden t-1)의 상태가 ht에 영향을 주고, ht는 ht+1에 영향을 주는 구조이다.  
그리고, 가중치 3개를 공유한다. (Wih, Whh, Who)

![img_6.png](img_6.png)  
RNN의 구조는 크게 **One to many, Many to many, Many to one**으로 나눌 수 있다.  
* One to many  
하나의 사진에 대해 여러 글자의 주석을 다는 image captioning을 예로 들 수 있다.  
  
* Many to many  
이어지는 말에 대한 (여러 음절) 여러 문자로 표현하는 STT (Speech To Text)를 예로 들 수 있다.  
  
* Many to one  
여러 문자에 대해 (예를 들면 단어로 이루어진 문장) 의미하는 바(하나)를 이해하는 Text classification을 예로 들 수 있다.  
  
### RNN 동작
![img_7.png](img_7.png)  
매 시점마다 인풋과 전 시점의 상태를 결합하여 tanh(하이퍼볼릭 탄젠트) 활성함수를 통해 나온 결과가 **다음 시점의 상태로써 대입**된다.  

![img_8.png](img_8.png)  
결합은 **concatenation 연산**이 수행된다.

### RNN 학습  
![img_9.png](img_9.png)  
RNN은 망이 깊어질수록 backpropagation을 진행하는 과정에서 gradient vanishing 문제가 발생할 수 있다.  

![img_10.png](img_10.png)  
그래서 일정한 그룹단위로 backpropagation을 수행하는 것을 **BPTT**(BackPropagation Through Time)이라고 한다.
![img_11.png](img_11.png)

### Bidirectional RNN  
![img_12.png](img_12.png)  
단방향으로 연결된 RNN은 위의 예시처럼 '거지'나 '지지'와 같은 글자를 구분하기 어렵다. (양방향 문맥 의존성)
고로 양방향으로 연결된 RNN은 문맥을 파악하여 이런 정보를 구별하는데 훨씬 유리하다.  

![img_13.png](img_13.png)  
위 문장에서 'are' 라는 단어는 앞 뒤 문맥을 보고 적절한 단어를 파악한다.

### 장기 문맥 의존성
![img_14.png](img_14.png)  
문장이 길어질수록 앞의 데이터와 뒤의 데이터의 연관성은 점차 낮아지는 현상이 생긴다.
이를 장기 문맥 의존성이라 한다. RNN에서는 이 현상이 gradient vanishing 현상으로 발현된다.  

