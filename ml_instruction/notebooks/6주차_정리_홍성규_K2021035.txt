<지난 시간 복습>
머신러닝은 크게 3가지 스텝으로 이루어져 있다.
첫번째는 전처리, 두번째는 알고리즘, 세번째는 후처리이다.
전처리에 해당하는 패키지들은 다음과 같다.

1. 전처리
1.1 encoding
from sklearn.preprocessing import LabelEncoder
from skelarn.preprocessing import OneHotEncoder

1.2 feature scaling
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing improt MinMaxScaler

1.3 학습 데이터 분리 & 교차검증
from sklearn.model_selection import train_test_split
from sklearn.model_selection import KFold
from sklearn.model_selection import StratifiedKFold

2. 알고리즘

3. 후처리
3.1 평가지표
from sklearn.metrics import confusion_matrix
from skelarn.metrics import accuracy_score
from skelarn.metrics import precision_score
from skelarn.metrics import recall_score
from skelarn.metrics import f1_score
from skelarn.metrics import roc_curve
from sklearn.metrics import roc_auc_score

<이번 시간 복습>
분류와 회귀의 차이
분류 : 학습데이터의 레이블 중 하나를 예측
회귀 : 연속된 값을 예측
=> 분류는 이산적인 결과를, 회귀는 연속적인 결과를 추론(inference)할때 사용된다.

분류의 알고리즘은 다음과 같다.
- 나이브 베이즈 : 베이즈 통계와 생성모델에 기반
- 로지스틱 회귀 (이름이 회귀라고 헷갈리지 말것) : 독립변수와 종속변수의 선형 관계성에 기반
- 의사결정 나무: 데이터 균일도에 따른 규칙 기반
- SVM (서포트 벡터 머신) : 개별 클래스 간의 최대 분류 마진을 효과적으로 찾아줌
- 최소 근접 알고리즘(Nearest Neighbor) : 근접거리를 기준
- 신경망: 심층 연결
- 앙상블 : 서로 다른 머신러닝 알고리즘 결합

앙상블 기법은 머신러닝 알고리즘들을 결합한 방법도 존재하나, Bagging, Boosting 기반 알고리즘도 존재한다.
Bagging ex) Random Forest
Boosting ex) Gradient Boosting, eXtra Gradient Boosting, LightGBM)
Stacking : 앙상블의 앙상블

앙상블의 기본 알고리즘은 일반적으로 "의사결정 나무"를 사용한다.
이번에는 의사결정 나무에 대해서 알아본다.

* 정보이득
정보이득 = 1 - 엔트로피 지수
엔트로피 : 물리적으로는 무질서도, 데이터 관점에서는 데이터 집합의 혼잡도 (서로 다른 값이 섞여 있으면 엔트로피가 높다.)

* 지니계수 (Gini index)
불평등 지수를 나타내는 계수
0일 때 가장 평등, 1일때 가장 불평등
의사결정 나무는 "지니 계수"가 낮아지는 방향으로 노드를 "반복적"으로 분할한다.

* 의사결정 나무 (Decision Tree)
의사결정 나무는 자료구조의 이진 트리 구조를 띄고 있으며, 최상단 노드는 "루트 노드", 말단 노드는 "리프 노드"라고 부르며, 그 사이에 존재하는 노드들은 "규칙 노드"라고 칭한다.
의사결정 나무의 노드는 크게 다섯가지 값들로 구성되어 있다.
1. 조건 (자식 노드로 분기하기 위한 조건 - 조건을 만족하는 경우 True의 방향인 왼쪽으로 자식 노드를, 불만족하는 경우False의 방향인 오른쪽으로 자식 노드르 뻗어 나간다.)
2. 지니계수 (위에서 언급했듯이 지니계수가 낮아지는 방향으로 자식 노드를 뻗어나간다.)
3. 데이터수 : 해당 노드에 존재하는 모든 데이터의 수를 의미한다.
4. class별 데이터 수 : 각 클래스별 몇개의 데이터가 존재하는지 표기한다.
5. 데이터 수가 가장 많은 class

의사결정 나무는 GraphViz 모듈을 통해 시각화를 진행할 수 있고, 해석이 가능하여 White box 알고리즘이라고 부른다. 고로, 딥러닝처럼 복잡한 특징 차원에 대한 문제를 해결할 수는 없을지라도 (성능이 상대적으로 떨어지더라도) 직관적으로 설명이 가능한 모델이기 때문에 현업에서 많이 사용된다.

* 의사결정 나무의 장단점
장점
- 직관적이다.
- 피처 스케일링 혹은 정규화 등의 전처리에 대한 의존도가 낮음.
단점
- 과적합이 잘 나타난다. (정확도를 높이기 위해서는 트리의 깊이가 점점 깊어진다.) 
트리의 크기를 사전에 제한하는 튜닝이 필요하다.

* 의사결정 나무의 파라미터
의사결정 나무의 클래스는 두 가지이다.
DecisionTreeClassifier : 분류를 위한 클래스
DecisionTreeRegressor : 회귀를 위한 클래스
DescisionTreeClassifier와 DecisionTreeRegressor의 파라미터는 동일하다.
min_samples_split(=2) : 노드를 분할하기 위한 최소의 샘플 수
mn_samples_leaf : 말단 노드가 되기 위한 최소한의 샘플 수
max_features(=None : 데이터 세트의 모든 피쳐를 사용해 분할 수행) : 최적의 분할을 위해 고려할 최대 피처 개수, int로 지정 -> 대상 피처의 수, float으로 지정 -> 대상 피처의 퍼센트
max_depth : 트리의 최대 깊이
max_leaf_nodes : 말단 노드의 최대 갯수

* GridSearchCV를 활용한 의사결정 나무의 하이퍼 파라미터 튜닝
머신러닝을 통해 조절할 수 없는 파라미터를 하이퍼 파라미터라고 한다.
GridSearchCV의 CV는 Cross Validation을 의미하며, 
조정할 하이퍼 파라미터의 집합을 GridSearchCV의 파라미터로 넘겨주어 최적의 학습된 모델을 파악할 수 있다.

ex)
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV

iris_data = load_iris()
X_train, X_test, y_train, y_test = train_test_split(iris_data.data, iris_data.target, 
                                                    test_size=0.2, random_state=121)

# 하이퍼 파라미터 집합
params = {'max_depth': [1, 2, 3], 'min_samples_split': [2, 3]}
dt_clf = DecisionTreeClassifier()
# cv : 폴드 수 / refit : 최적의 하이퍼 파라미터를 찾은 뒤, 입력된 estimator 객체를 해당 하이퍼 파라미터로 학습
grid_dt = GridSearchCV(dt_clf, param_grid=params, cv=3, refit=True)
grid_dt.fit(X_train, y_train)

# 각 하이퍼 파라미터 조합의 케이별 결과
# ex) (1) max_depth = 1, min_samples_split = 2 (2) max_depth = 1, min_samples_split = 3, ...
scores_df = pd.DataFrame(grid_dtree.cv_results_)

* 의사결정 나무의 피처 중요도
의사결정 나무의 장점으로 어떤 특징이 중요한지 쉽게 파악할 수 있다.

import seaborn as sns

# feature importance를 column 별로 시각화 하기 
sns.barplot(x=dt_clf.feature_importances_ , y=iris_data.feature_names)
